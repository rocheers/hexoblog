{"pages":[{"title":"About me","permalink":"http://yuannow.com/about/index.html","text":"Yuan Zhang"},{"title":"Gallary","permalink":"http://yuannow.com/gallary/index.html","text":"第一波 第二波未完待续……"},{"title":"Links","permalink":"http://yuannow.com/links/index.html","text":""}],"posts":[{"title":"用Python在Hadoop上跑MapReduce","permalink":"http://yuannow.com/2017/09/29/Map-reduce-works-perfectly-with-Python/","text":"本文目的这篇文章主要会给大家介绍一下如何将Python和Hadoop结合起来工作。有接触过MapReduce的朋友都知道，Hadoop的运行环境主要是Java，一般介绍Hadoop和MapReduce的教程和书籍也都是基于Java的。因为我个人对Java并不太感冒，一直以来钟情于Python的简洁实用理念，同时又对MapReduce有兴趣，因此萌生了Python的MapReduce结合的想法。本文也是我经过Google学习他人教程，以及自己实际练习得出来的一些心得，在此分享给各位。 环境搭建首先，你需要有个Hadoop的运行环境，还有Python运行环境。本文主要目的不在分享安装环境，因此有从零开始的朋友，可以先去百度或者Google上搜一下相关教程。下面分享几个相关的教程： Hadoop: Setting up a Single Node Cluster Hadoop安装教程_单机/伪分布式配置_Hadoop2.6.0/Ubuntu14.04 使用Docker在本地搭建Hadoop分布式集群 MapReduce in Python下面我就来看Python里如何实现mapper和reducer。 mapper.pymapper要做的工作就是从stdin里读取数据，然后分割成&lt;key, value&gt;的pair。这里以最基础的word count为例，key就是指文章中拆出来的词，value就是指每个词的个数。mapper是不会将相同的词的个数进行统计加和的，那是reducer的工作，因此mapper的输出就是由很多行&lt;key&gt; 1组成，下面会看到程序运行实际的结果。 mapper.py1234567891011#!/usr/bin/env python3import sys# 从stdin中获取输出信息for line in sys.stdin: words = line.strip().split(\" \") for word in words: # output format: word'\\t'1 print(\"&#123;0&#125;\\t&#123;1&#125;\".format(word, 1)) reducer.py上面提到了，mapper只进行词汇分割，计数为1的工作，那么reducer就是用来将相同词汇的出现次数进行统计加和的工作。同mapper一样，reducer也是从stdin中获取输入，然后将结果输出到stdout。 reducer.py12345678910111213141516171819202122#!/usr/bin/env python3import syslast_word = Nonetotal_count = 0for line in sys.stdin: word, count = line.strip().split(\"\\t\") count = int(count) # 如果当前的word不等于上一个，说明开始新的了 if last_word and last_word != word: # 这里输出结果，相当于把结果写进stdout print(\"&#123;0&#125;\\t&#123;1&#125;\".format(last_word, total_count)) total_count = 0 last_word = word total_count += count# 不要忘记最后一个word的输出if last_word: print(\"&#123;0&#125;\\t&#123;1&#125;\".format(last_word, total_count)) 本地测试我们先来本地测试一下正确性。12345678YuanMBP:src Vergil$ echo \"东 南 西 北 中 发 白\" | demo/mapper.py 东 1南 1西 1北 1中 1发 1白 1 再来看一下reducer：12345678YuanMBP:src Vergil$ echo \"东 南 西 北 东 中 东 发 东 白\" | demo/mapper.py | sort | demo/reducer.py 东 4中 1北 1南 1发 1白 1西 1 请注意，这里我在运行mapper和reducer之间加入了一个sort，这是必须的，了解map-reduce工作原理的朋友应该都明白这里为什么有一个sort，如果不加的话，我们的东风杠就识别不了啦。我们在写mapper和reducer的时候是不需要关注它排序的问题，因为Hadoop中的map-reduce会自动进行排序。1234567891011YuanMBP:src Vergil$ echo \"东 南 西 北 东 中 东 发 东 白\" | demo/mapper.py | demo/reducer.py 东 1南 1西 1北 1东 1中 1东 1发 1东 1白 1 在Hadoop上跑程序准备测试数据我的运行环境是在Docker上搭建的，首先我们需要先把用来测试的文章放到HDFS里12345root@hadoop-master:~/src/demo# hdfs dfs -put words1.txt /inputroot@hadoop-master:~/src/demo# hdfs dfs -ls /inputFound 1 item-rw-r--r-- 2 root supergroup 127 2017-09-30 03:57 /input/words1.txtroot@hadoop-master:~/src/demo# words1.txt1234Let me write down something trivialsomething that is not importantsomething that looks like bullshityes that is what I want 我就随便写了几句放在words1.txt里，看看运行结果是否正确。 运行MapReduce用过Java版Hadoop的朋友，应该还有印象如何编译运行吧，其实就和运行Java程序的过程很像。但这里用Python来执行，就稍微有些不太一样了。首先我们需要用到一个hadoop-streaming-2.x.x.jar这样的一个工具，这里xx代表版本号。它的具体解释可以参考Hadoop官方给的Document，我这里就做个简单的介绍。Hadoop Streaming是Hadoop提供的一个工具，可以让你以任意的可执行程序或脚本，来创建和运行MapReduce，这里官网给了一个简单的例子：12345$HADOOP_HOME/bin/hadoop jar $HADOOP_HOME/hadoop-streaming.jar \\ -input myInputDirs \\ -output myOutputDir \\ -mapper /bin/cat \\ -reducer /bin/wc 现在，我们来用Hadoop Streaming来运行自己的程序。1234567root@hadoop-master:~/src/demo# hadoop jar ../hadoop-streaming-2.7.2.jar \\&gt; -input /input \\&gt; -output /output \\&gt; -mapper mapper.py \\&gt; -reducer reducer.py \\&gt; -file mapper.py \\&gt; -file reducer.py 这里有两点需要注意： 后面的两个-file是必须要加的，否则程序无法顺利运行； mapper.py和reducer.py要提前记得赋予它们可执行的属性。 接下来，我们来验收一下程序的结果：1234567891011121314151617181920212223root@hadoop-master:~/src/demo# hdfs dfs -ls /outputFound 2 items-rw-r--r-- 2 root supergroup 0 2017-09-30 04:31 /output/_SUCCESS-rw-r--r-- 2 root supergroup 128 2017-09-30 04:31 /output/part-00000root@hadoop-master:~/src/demo# hdfs dfs -cat /output/part*I 1Let 1bullshit 1down 1important 1is 2like 1looks 1me 1not 1something 3that 3trivial 1want 1what 1write 1yes 1root@hadoop-master:~/src/demo# 试一下多文件看看有没有问题，我将words1.txt复制出一模一样的两份，也就是现在有三份相同的输入文件，再来跑一遍试试：123456root@hadoop-master:~/src/demo# hdfs dfs -rm -r /output17/09/30 04:48:37 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.Deleted /outputroot@hadoop-master:~/src/demo# ./run_script.sh /input /output mapper.py reducer.pyRunning python in Hadoop by hadoop streaming... 这里我自己做了run_script.sh这样子一个shell script，用来缩短执行命令的长度，不然每次都要输入那么长真的好麻烦……123456789101112131415161718root@hadoop-master:~/src/demo# hdfs dfs -cat /output/part*I 3Let 3bullshit 3down 3important 3is 6like 3looks 3me 3not 3something 9that 9trivial 3want 3what 3write 3yes 3 看上去没有任何问题。 后记更多的关于Hadoop Streaming的内容，还希望大家去官网文档中查阅。例如像分配map和reduce的数量，设置partitioner，这样的参数都可以通过Hadoop Streaming来调整，还是很有意思的。 在下一篇关于MapReduce的文章中我会介绍一个相比较于word count复杂一点的例子，依然是用Python和Hadoop的结合。 Related Links Writing an hadoop mapreduce program in python Hadoop Streaming Documentation"},{"title":"Migrated from Heroku to Netlify","permalink":"http://yuannow.com/2017/09/28/Netlify/","text":"之前学习ruby on rails的时候，开始接触的Heroku。这次部署个人博客，想着Heroku应该是个不错的选择吧，但试用了几天的Heroku之后，我决定还是转到其他地方部署我的blog，为什么呢？原因有二： 免费的Heroku App，如果搁置时间长了一直无人访问的话，再次访问是需要一定时间等待激活的，就像是电脑睡眠了等着叫醒一样。付费的话太贵，对于只是搭载个人博客而言不太划算； Heroku服务器是在美国和欧洲，而且一个App只能存在在某一个区域，不可变。所以对于国内的朋友，登录我的博客有时会有不小的延迟，虽然对于一个静态网站，本来也无需很快的响应速度，但配合第一条，这个时间有时真的让人无语…… 那之所以选择Netlify也是有两个原因： 不存在Heroku的第一个问题； 静态内容部署采用的是global CDN方式，这样用户登录博客的时候会根据用户所在地选取最近的节点获取信息。 除了这两点以外，Netlify的设置界面也做得十分简洁易用，相比Heroku而言，甚至连部署都更加简单，除了利用git部署，还可以直接把生成好的静态网页文件夹拖动到一个部署框内"},{"title":"Hello World","permalink":"http://yuannow.com/2017/09/26/hello-world/","text":"前言终于将个人博客搭建起来了……虽然听说Hexo搭建博客十分容易，但对于前端完全不了解的我，颇费了一番功夫。 Hexo初始化这个博客目录时，自带了一个以“Hello World”为标题的文章，考虑到“Hello World”是作为学习编程代码第一课，作为一个标准码农的我就决定沿袭这个传统，从此开始自己的博客之旅。 Hello Hexo Hello World"}]}