<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[Selenium和BeautifulSoup的简单应用：爬取Pythonbooks.org]]></title>
    <url>%2F2017%2F11%2F06%2FPython-Books-Crawler%2F</url>
    <content type="text"><![CDATA[使用Python是个很有意思的体验，语法相对简单，各种包的支持也非常到位，因此无论用Python解决什么问题，都觉得弹指间可以让其灰飞烟灭。作为一个工具，一种编程语言，在帮助不同人群实现各种有趣的小目标这点上，Python真的是无出其右。 本文目的Python中有一种很好玩的应用，就是爬虫，简单说就是通过它，用户可以抓取网页上的信息。比如，在本篇文章中，我就将介绍自己爬取Pythonbooks网站的过程，以此来让各位对爬虫有个大概的认识，并且描述过程中我也会顺带介绍一下Selenium和BeautifulSoup两个爬虫中常用包的用法。 在Python的学习过程中，我相信每一位“Pythoner”都会经历这样一个过程：市面上这么多本Python的书籍和教程，究竟哪一个才是最好的呢？国外有个叫Dibya Chakravorty的哥们在当初学习Python的时候也思考到这个问题，于是他就做了一个网站，Pythonbooks.org，专门来统计不同应用领域下的Python书籍，以及它们的评分，评分是根据书的销售情况来计算的。所以今天我就用爬虫的方式来带各位看一下他网站上是怎么说的。 环境需求我是在Python3下运行的爬虫，用到的包就两个：Selenium和BeautifulSoup，安装最新版就可以。 下面我们先来看一下这两个包究竟都是做什么的，为什么我会用到它们。 Selenium和BeautifulSoup打开Selenium官网，第一句话就是 Selenium automates browsers. 。没了，就这一句，就说明了Selenium的作用，它就是用来模拟操作浏览器的工具，通过API，用户可以用代码来操作浏览器上网页的跳转，关闭，填写表单等操作。 那BeautifulSoup的作用呢？以下引用Wikipedia： Beautiful Soup is a Python package for parsing HTML and XML documents (including having malformed markup, i.e. non-closed tags, so named after tag soup). It creates a parse tree for parsed pages that can be used to extract data from HTML, which is useful for web scraping. 所以BeautifulSoup的作用就是用来从HTML和XML等文件中提取所需信息。BeautifulSoup实际上是bs4模块的一个子模块，bs4的作用极其强大，可以针对各种内容分割和获取信息，BeautifulSoup是其专门针对HTML和XML的。因为本文只用到BeautifulSoup，因此其他的模块内容就暂不介绍了，有兴趣的朋友可以自行查阅。 简单介绍两个工具包之后，接下来让我们开始网站的爬取工作吧。 网站爬取在正式爬取之前，我们还有一个问题需要解决，就是这个网站的规模和动态程度，如果只是纯静态网页，而且只要爬取一个页面，那其实很容易。或者如果只要爬取3、5个网页，那我们完全可以手动跳转每个网页然后运行一遍程序，也不过就运行3、5遍而已，但当网页数量比较多的时候，这么不自动化的办法就显得很麻烦。 这个网站上已经针对不同领域，为用户区分好了所有的Python书籍归属种类。这点真的非常方便，这样我就只需要按照他网站给出的分类分别爬取下来就可以了，但这里种类的繁多，一个网页一个网页的手动跳转就很麻烦了，所以这里就需要Selenium来模拟鼠标的点击操作，进行网页的跳转，然后用BS爬取内容。这就是整个爬取过程的初步思考。 初始化webdriver1234def init_driver(): driver = webdriver.Firefox() # driver.wait = WebDriverWait(driver, 5) return driver 用Selenium来模拟浏览器的各种行为，就需要先进行webdriver的初始化，如上面的代码所示。中间注释掉的那一行可加可不加，它的作用主要就是考虑到有些网页是动态呈现内容，在输入网址之后加载网页到完全加载完毕可能需要等待一段时间，WebDriverWait就是用来告诉driver可以考虑的等待时间范围。一般来说都会搭配该类下的unitil、until_not方法和expected_conditions使用，本文用不到这些内容，所以就不深讲了，有兴趣的朋友可以看这里 爬取分类从这里开始，我们就要正式爬取网站内容了。首先需要先把所有的分类获取，这样也方便我们最后整理信息，无论是以哪种方式存储。 123456789101112131415161718192021url = "http://pythonbooks.org/"menu = ["Intermediate", "Topical", "Others"]categories = &#123;'Beginner': 'http://pythonbooks.org/for-programming-beginners'&#125; def crawl_category(driver, url, categories, menu): driver.get(url) for m in menu: sub_menus = &#123;&#125; driver.find_element_by_link_text(m).click() # 停顿一下以等待下级菜单的弹出 time.sleep(0.2) try: sub_menu_block = driver.find_element_by_xpath("//li[@class='dropdown open']") sub_menu_name = sub_menu_block.find_element_by_tag_name("ul") for sub_menu, sub_menu_link in zip(sub_menu_name.text.split('\n'), sub_menu_name.find_elements_by_css_selector('a')): sub_menus[sub_menu] = sub_menu_link.get_attribute('href') except NoSuchElementException: pass categories[m] = sub_menus 这里有细心的朋友可能看到我一开始给了一个categories的字典变量，里面有内容，而不是空的。这里之所以这样做，是因为在网页上显示的四个总分类中，只有Beginner是没有下级菜单的，所以这里就单独处理一下。 代码中driver.find_element_by_xpath(&quot;//li[@class=&#39;dropdown open&#39;]&quot;)，这句就是获得了弹出的下级菜单中的内容。具体的对照请看下图： for循环中，两个遍历的变量，前者是获取的分类的string值，后者是对应的链接地址。我将它们都存进一个字典中，这样在下面爬取内容的时候，只需要对该字典遍历一遍就可以了。 爬取书籍信息上面我们已经将书籍分类都汇总好了，下面就开始挨个网页的爬取书籍的信息。这里书籍的信息我主要就爬取了五个：标题，作者，出版日期，分数，和封面图像。为了存储和打印方便，我就讲书籍信息封装成一个类，如下所示： 12345678910class BookInfo(): def __init__(self, title, author, score, pub_date, href): self.title = title self.author = author self.score = score self.pub_date = pub_date self.href = href def __str__(self): return "Book '&#123;&#125;' written by &#123;&#125;, &#123;&#125;. Popularity score: &#123;&#125;".format(self.title, self.author, self.pub_date, self.score) 接下来就是对页面的爬取工作，先贴代码： 12345678910111213141516171819202122232425def crawl_book_info(driver, url): driver.get(url) html = driver.page_source # 等待网页加载分数 time.sleep(2) soup = BeautifulSoup(html, 'html.parser') book_blocks = soup.find('div', id='result-content') book_block_list = [] for book_block in book_blocks.find_all(class_='row book-wrapper-row'): book_block_list.append(book_block) book_list = [] scores = driver.find_elements_by_xpath("//div[@class='bar']") for book, score in zip(book_block_list, scores): title = book.find('h2').string pub_date = book.find('span', class_="publication-date").string.strip() author = book.find('h3').string.strip()[3:] href = book.find('img')['src'] book_list.append(BookInfo(title, author, score.text, pub_date, href)) return book_list 内容比较多，我们一点点来看。首先我们看看每个字段的位置。 我在一开始做这部分的时候是想用BS就可以了，因为在HTML中各元素的位置一目了然，爬取十分方便。但后来发现分数这一项，死活爬不下来，每次用BS去找这一块，返回的都是空。猜测可能是分数是动态生成的关系，因此BS抓取不到，具体内在原因没有去详细了解，如果有大神知道是怎么回事，还希望不吝赐教。后来我实在没有办法，就只好改用Selenium来抓取，driver.find_elements_by_xpath(&quot;//div[@class=&#39;bar&#39;]&quot;)。正因为分数是动态生成，所以在加载网页之后需要给一个延迟，等一下分数的刷新。分数和书籍是一一对应的，所以在for循环中用zip对两者一起进行协同遍历就可以了。 存储书籍信息我们已经将书籍信息全部抓取下来，下面就是要考虑如何存储的问题。我这里是将所有的书按照分类（如果有第二级分类，就按照第二级）存储到一个markdown文件里。markdown最大的有点是，很方便转成html文件，这样既利于存储也利于展示。代码如下，因为这部分比较简单，我想就不用解释太多了。 12345678910111213141516171819def book_save(books_list, cat, sub_cat=None): # 如果存在二级分类，就把一级二级拼起来作为文件名 title = str(cat) + '-' + str(sub_cat) if sub_cat else str(cat) with open('../Pythonbooks-' + title + '.md', 'w') as file: if not sub_cat: file.write("# &#123;&#125;\n\n".format(cat)) else: file.write("# &#123;&#125; - &#123;&#125;\n\n".format(cat, sub_cat)) num = 1 for book in books_list: file.write("## &#123;&#125;. &#123;&#125;\n".format(num, book.title)) file.write("![&#123;&#125;_cover](&#123;&#125;)\n\n".format(book.title, book.href)) file.write("Author: &#123;&#125;\n\n".format(book.author)) file.write("&#123;&#125;\n\n".format(book.pub_date)) file.write("**Popularity score: &#123;&#125;**\n\n".format(book.score)) file.write("------------\n") num += 1 主函数：功能串联主函数主要就是将各个部分串联起来，具体代码如下： 123456789101112131415161718192021222324252627if __name__ == "__main__": url = "http://pythonbooks.org/" menu = ["Intermediate", "Topical", "Others"] categories = &#123;'Beginner': 'http://pythonbooks.org/for-programming-beginners'&#125; driver = init_driver() crawl_category(driver, url, categories, menu) all_books = &#123;&#125; for key, value in categories.items(): # 如果一级分类的value是字典的话，那就说明还存在二级分类 if key not in all_books and not isinstance(value, str): all_books[key] = &#123;&#125; try: for k, v in value.items(): book_list = crawl_book_info(driver, v) all_books[key][k] = book_list book_save(book_list, key, sub_cat=k) except: book_list = crawl_book_info(driver, value) all_books[key] = book_list book_save(book_list, key) driver.quit() 在所有的分类中，只有Beginner是没有二级分类的，因此需要对它进行单独处理。对应到代码中，就是except下面的内容。 我在主函数里还设置了一个all_books的变量，但其实并没有用到。因为一开始我本打算先存到一个字典变量中在，再对该变量进行遍历，一个一个将书籍信息存到文件中。后来发现其实并不需要，完全可以边爬取边存。 好了，所有的爬取内容就都结束了，当你运行程序之后就会看到Firfox浏览器自动弹出，然后逐个页面的自己跳转，最后浏览器关闭的时候，你也就收获了一大堆的md文件。 md的内容如下图所示： 将md渲染成HTML后在网页中显示是这样的： 结语相对来说，这次的爬取任务很简单，因为网站没有反爬机制。其实在爬取过程中，最难的，最体现斗智斗勇的地方，就是反爬与反反爬，这方面我不是很了解。因为一般用到爬虫的时候都是确实切身需要的时候，并非为了技术而去钻研，所以大部分时候爬的都是一些明面上的可搜集的数据。网上关于Python爬虫的介绍和例子其实有很多很多，现如今，除了数据分析，机器学习这部分，恐怕大多数人都是因为爬虫的趣味性和实用性在学Python吧。 在本文的最后，我给出了几个很有帮助的链接，对于想要学习Selenium和BeautifulSoup的朋友，值得一看。 Related Links Python Selenium——一定要会用的Selenium的等待，三种等待方式解读 selenium + python 中文文档 Beautiful Soup 4.2.0 中文文档 Beautiful Soup 的用法 Python爬虫利器五之Selenium的用法]]></content>
      <categories>
        <category>Web Application</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>Selenium</tag>
        <tag>BeautifulSoup</tag>
        <tag>crawler</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[用Python在Hadoop上实现搜索自动补全]]></title>
    <url>%2F2017%2F10%2F09%2FImplement-Search-Auto-Complete-by-Python-Hadoop%2F</url>
    <content type="text"><![CDATA[记得很早的时候，刚开始接触搜索引擎，那时候Google还没有被墙，百度也是刚刚诞生没多久，无论我在输入框内输入什么，只是显示我输入的内容，并没有任何联想功能提示我接下来可能匹配的内容。有时候想找个东西，可就是死活想不起来准确的名字或者描述方式，这个时候搜索引擎也帮不了你，你只能变着法的尝试各种输入，来寻求最终你想要的结果。后来搜索引擎加入了联想功能，这着实是一大进步，极大地提高了搜索效率。有的时候，我们对于想查询的某一问题的提问方式是很模糊的，这个时候当你输入了一些关键词，发现搜索引擎会根据你的输入给你一些后续内容的建议，会让你更容易的找到自己想要的答案。 本文目的上一篇我在《用Python在Hadoop上跑MapReduce》中介绍了一些关于如何利用Hadoop Streaming运行Python版MapReduce的简单操作，如何实现词频统计就像是MapReduce中的Hello World，不过做完了入门教程，为了深入学习，还得需要更多的练习。 在这篇文章中，我将会带各位实现一个很简单的搜索联想功能，比较粗糙，但是看起来也挺像那么回事的~ 正文为了实现autocomplete，首先需要搞清楚它背后的原理，搜索引擎究竟是根据什么来给出提示的？比如我输入一个autocomplete之后，为什么后面推荐的是python，algorithm之类的，而不是Naruto，One Piece这些呢？ 无论是百度还是谷歌，其实在大家用它们的搜索引擎收集你需要的信息的时候，它们也同样在收集信息。它们收集了大量的用户搜索的输入信息，同时还在抓取各种网页内容的信息，简单来说，搜索引擎通过某种算法将各种收集到的资料综合到一起，最终给出了一个输入联想的列表，这就是自动补全功能。 私下里作为学习MapReduce的项目，我们肯定不可能去试图实现真正的搜索引擎。不过做一个简陋版的，帮助自己理解autocomplete背后的实现原理，以及如何利用MapReduce来实现，也是足够的。 N-Gram对于autocomplete背后实现的原理，很重要的一点就是N-Gram，这篇文章有简明的介绍，举个例子好了：implement search autocomplete in python 这句话如果按2-Gram划分的话，就是implement search，search autocomplete，autocomplete in，in python； 那么按照3-Gram划分的话，就是implement search autocomplete，search autocomplete in，autocomplete in python； 按照4-Gram划分的话，就是implement search autocomplete in，search autocomplete in python； 按照5-Gram划分的话，就是一整句话，不作任何切割。 这里我没有给出1-Gram的例子，各位可以思考一下是为什么，这里先卖个关子，下面会提到原因。这里我们先回头思考一下搜索引擎给出的联想词汇，比如我输入了一个autocomplete，那么后面你觉得是给出in python看上去更自然还是Transformer更合理？显然是前者对么，如果给出了后者，你可能会想：autocomplete Transformer是什么鬼？！难道是某种新型变形金刚么？没听过啊！那之所以会有这样的感觉，是因为从我们平时获取的信息中，我们对于autocomplete这个词后面的衔接词接收到的也是in python远远多于Transformer，对吧？所以说到这里就大概有点明晰了，搜索引擎对于联想词汇的推荐也有这方面的考虑，我们给输入一个或多个引导词，autocomplete功能将后面最有可能出现的几种选择展现给我们。所以这也是为什么用到N-Gram的原因，这里其实就是一个句子分割的过程。 那么这里就引出了下一个问题：是该基于一句话最后一个词来预测后面的推荐，还是根据更多的词甚至一整句来预测下面的出现呢？ 还继续以上面的例子说好了。假设我已经输入了implement search autocomplete in，如果是按照最后一个出现的词来进行联想，那么in后面其实可以跟很多很多内容，都不会违和，对吧？比如in box，in air，in heart等等，但如果把这些联想放到一整句中，显然是不太合理的。因此，我们在实现autocomplete的时候，其实是基于N-Gram预测N-Gram。 那到这里，其实autocomplete的实现的大致思路就应该有了。我们需要将大批量的文档用N-Gram的处理方式进行切割，统计相同短语出现的频次，构建N-Gram的模型，然后找出每个短语之后出现频次最高的几个词汇作为预测保存下来。最终将结果存入数据库，以方便之后调用。下面我们就来看看如何一步步具体实现。 MapReduce实现N-Gram模型构建在这次的代码实现中，我们需要用到两个mapper和reducer。首先，我们需要一对mapper和reducer将输入的文档进行切割。 n_gram_mapper.py 12345678910111213141516171819202122232425262728import sysimport stringimport re def main(N_Gram): for line in sys.stdin: # 去除所有的标点符号 translator = str.maketrans(string.punctuation, " " * len(string.punctuation)) line = line.translate(translator).strip().lower() # 去除所有的数字 line = re.sub("\d", " ", line) # 按照空格或者\t来切割 words = re.split("\\s+", line) if len(words) &lt; 2: continue for i, word in enumerate(words): result = word for j, next_word in enumerate(words[i+1:], 1): if j &lt; N_Gram: result += " " result += next_word print("&#123;&#125;\t&#123;&#125;".format(result, 1)) if __name__ == '__main__': main(5) 如果你想构建一个N=5的N-Gram模型，那么在mapper里，就需要将2到5的分割方式都输出。还记得上面我有留一个悬念，为什么不考虑1-Gram么？因为显然1-Gram是没有用的，它并不能告诉我一个短语后面可能出现的词有什么。 n_gram_reducer.py 1234567891011121314151617181920import sysfrom itertools import groupbyfrom operator import itemgetter def parse_output(std_input): # 利用generator节省MapReduce内存使用空间 for line in std_input: yield line.strip().split("\t") def main(): data = parse_output(sys.stdin) for word, group in groupby(data, itemgetter(0)): try: total_count = sum(int(count) for word, count in group) print("&#123;&#125;\t&#123;&#125;".format(word, total_count)) except ValueError: continue if __name__ == '__main__': main() 这回的reducer我用到了generator来调用数据，这样做可以有效节省内存占用空间。因为随着输入数据量的越来越大，即便是5-Gram，依然也是个很庞大的记录数量。下面的主函数部分，我也放弃了之前的简单用法，用groupby可以明显提升代码的阅读逻辑，对itertools.groupby不太熟悉的朋友可以看这份官方文档。 N-Gram测试结果做下一步之前，我们先来本地测试一下这一对mapper和reducer是否好用。 测试文档：我直接把MapReduce的Wiki里头一段关于它的说明解释，粘贴下来作为测试文档用。1234567891011121314151617181920212223root@hadoop:~/src/p2py# cat input/file2.txt | ./n_gram_mapper.py | sort | ./n_gram_reducer.pya cluster 1a good 1a good mapreduce 1a good mapreduce algorithm 1a map 1a map procedure 1a map procedure method 1a map procedure method that 1a mapreduce 1a mapreduce program 1a mapreduce program is 1a mapreduce program is composed 1...and reduce 3and reduce capabilities 1and reduce functions 2...map and 3map and reduce 3map and reduce capabilities 1map and reduce functions 2... 我这里就截取一部分结果，实在太长了…… 构建预测概率的模型我们现在已经有了N-Gram切割后的结果了，下一步就是要在此基础上分析每一个词汇或者短语后面可能出现的内容，这里其实可以理解成构建一个概率模型，很简单一个概率模型。 举例说明，我们先来看刚才的结果生成的一部分123456789and data 1and development 1and fault 3and generating 1and less 1and providing 1and reduce 3and scatter 1and sorting 1 先仅仅分析and之后可能出现的词汇，所有的情况都在这里摆着了。那这时候autocomplete会如何给出推荐呢？很显然的，fault和reduce会放到头两个推荐对吧，为什么？这其实就是个概率的问题：and后接一个词在文中出现了1+1+3+1+1+1+3+1+1=13次，那fault和reduce推荐的概率就是3/13，剩下的所有都是1/13的概率。对于autocomplete系统来说，这意味着当用户输入了and之后，它认为用户更有可能继续输入的是fault和reduce，因为从它以往经验（系统所得到的输入）来看，fault和reduce出现的频次更多一些，相比较于其他的结果。因此，下一步工作，我们需要得到一个类似于and&lt;\t&gt;data=1这样的数据记录样式，来统计所有的短语之后跟随的词汇以及它出现的频次。这里我们之所以不用概率来进行记录，是因为从刚才的计算过程来看，词频和概率是正相关的，那么就没必要多算一步记录概率了。 prob_mapper.py 1234567891011121314151617import sysimport string def main(threshold): for line in sys.stdin: words_phrase, count = line.strip().split("\t") if int(count) &lt; threshold: continue words = words_phrase.strip().split(" ") if len(words) &lt; 2: continue print("&#123;&#125;\t&#123;&#125;".format(' '.join(words[:-1]), words[-1] + "=" + count)) if __name__ == '__main__': main(2) mapper的工作其实很简单，基本没有什么需要说明的。需要注意的是，这里我加入了一个threshold参数，意义是为了筛选掉一部分出现频次太低的结果。刚才举的例子里，每个结果出现的频次其实都不高，这样threshold肯定是没用的，但实际生产中，比如像Google和百度这样的超大规模的搜索引擎，每天可能抓取的数据量十分庞大。事实上，我们每次在搜索框中输入内容，得到的联想其实都在二十条以内，一般来说不会给太多的，太多的话，用户筛选起来也是个麻烦。那对于那些基本很少出现的词组组合，也就没必要存储下来，被搜索到的概率太低，如果全部都记录下来，对数据库的存储容量是个很大的负担。 prob_reducer.py 12345678910111213141516171819202122232425262728import sysfrom itertools import groupbyfrom operator import itemgetter def parse_output(std_input): for line in std_input: yield line.strip().split("\t") def main(n_gram): data = parse_output(sys.stdin) for starting_phrase, group in groupby(data, itemgetter(0)): result = &#123;&#125; for _, word_count in group: word, count = word_count.split("=") count = int(count) if count not in result: result[count] = [] result[count].append(word) i = 0 for key, value in result.items(): if i &lt; n_gram: for word in value: print("&#123;&#125;,&#123;&#125;,&#123;&#125;".format(starting_phrase, word, key)) i += 1 if __name__ == '__main__': main(4) starting_phrase代表的是用户输入的部分，following_word代表的是后面可能出现的词汇，count顾名思义就是指词频了。在reducer中，我们先按照词频的不同将可能出现的词汇分组放置，然后再根据我们需要的N-Gram大小来依次输出。这里的参数n_gram和之前n_gram_mapper里的n_gram意思一样，但取值可以不同。 预测概率的测试结果下面我们来看看本地测试结果吧，就用上一步n_gram_reducer得出的结果继续操作。 12345678910111213141516171819202122232425262728root@hadoop:~/src/p2py# cat result.txt | ./prob_mapper.py | ./prob_reducer.py and,fault,3and fault,tolerance,3and,reduce,3and reduce,functions,2big,data,2communication,cost,2fault,tolerance,3is,a,2map,and,3map and,reduce,3map and reduce,functions,2mapreduce,framework,3method,that,2method that,performs,2model,and,2model,is,2not,the,2of,the,4of the,mapreduce,2of the mapreduce,framework,2optimizing,the,2reduce,functions,2such,as,2that,performs,2the,mapreduce,4the mapreduce,framework,3the,various,2 因为之前在mapper里我们将threshold设置为2，这里我们就可以看到结果中只有词频不小于2次的。 运行在Hadoop上刚才在本地跑过了之后，下面来进行Hadoop上的测试。为了方便快捷，我编写了一个script来运行两对mapper和reducer，最后把得到的数据从HDFS里导出到本地。123456789101112131415161718192021222324252627root@hadoop:~/src/p2py# ./run_script.sh Cleaning old results in /output...17/10/10 01:30:20 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.Deleted /output Running python in Hadoop by hadoop streaming... packageJobJar: [/tmp/hadoop-unjar5398292596529368576/] [] /tmp/streamjob5913697815224381390.jar tmpDir=null17/10/10 01:30:22 INFO client.RMProxy: Connecting to ResourceManager at hadoop-master/172.18.0.2:803217/10/10 01:30:22 INFO client.RMProxy: Connecting to ResourceManager at hadoop-master/172.18.0.2:803217/10/10 01:30:23 INFO mapred.FileInputFormat: Total input paths to process : 1...17/10/10 01:30:49 INFO streaming.StreamJob: Output directory: /output/first Running 2nd mapper and reducer... packageJobJar: [/tmp/hadoop-unjar6814698868258273333/] [] /tmp/streamjob6470458846508985375.jar tmpDir=null17/10/10 01:30:51 INFO client.RMProxy: Connecting to ResourceManager at hadoop-master/172.18.0.2:803217/10/10 01:30:51 INFO client.RMProxy: Connecting to ResourceManager at hadoop-master/172.18.0.2:803217/10/10 01:30:52 INFO mapred.FileInputFormat: Total input paths to process : 1...17/10/10 01:31:13 INFO streaming.StreamJob: Output directory: /output/second Moving outputs from HDFS to local... Got the output! 一切顺利运行！最终结果也被成功导出到本地。 导入数据库其实到上一小节，MapReduce的工作就都做完了，但为了让autocomplete可以展现出应有的效果，这里还需要将刚才生成的数据导出到数据库中，以便之后和Web结合来体现功能。 数据库我用的是MySQL，大家可以任意选择。下面是我的数据库操作代码：1234567891011121314151617181920import sysimport MySQLdb as mdb def read_data(file): for line in file.readlines(): yield line.strip().split(",") def main(input_data): with mdb.connect('localhost', 'username', 'password', 'dbname') as cur: cur.execute("DROP TABLE IF EXISTS output") cur.execute( "CREATE TABLE output(starting_phrase VARCHAR(250), following_word VARCHAR(250), count INT)") with open(input_data, 'r') as file: for starting_phrase, following_word, count in read_data(file): cur.execute( "INSERT INTO output(starting_phrase, following_word, count) \ VALUES('&#123;&#125;', '&#123;&#125;', &#123;&#125;)".format(starting_phrase, following_word, count)) if __name__ == '__main__': main("output") 数据库中显示的结果： 展示结果我简单做了一个Web展示的页面，基于Ajax和PHP，连接数据库后，测试结果如下图所示： 总结本文从零开始，不太详细的介绍了autocomplete的工作原理，以及如何利用Python和MapReduce来处理数据。相比较Java实现这些内容而言，Python确实需要注意更多的细节，毕竟Java是Hadoop原生环境，configuration的配置真的是方便。之前做wordcount时没有觉得Python+Hadoop Streaming的方式有什么问题，因为Python天生的简洁特质，感觉比Java啰里啰嗦的舒服多了。但这次的代码实现上就看出端倪了，Python下更多的细节部分需要开发者自己写代码去维护，就像不同的mapper，reducer之间的数据传输，还有输出到数据库保存。 一路从头看到这里的朋友，感谢你的阅读，如果有疑惑，欢迎👇下面留言，如果文章中有什么不对的地方，也欢迎批评和指正。 Related Links N-gram的原理、用途和研究]]></content>
      <categories>
        <category>Big Data</category>
      </categories>
      <tags>
        <tag>Hadoop</tag>
        <tag>Python</tag>
        <tag>MapReduce</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Data analysis in Python by Pandas]]></title>
    <url>%2F2017%2F10%2F02%2FData-analysis-in-Python-by-Pandas%2F</url>
    <content type="text"><![CDATA[Python在数据科学领域的应用真的是越来越普及，得益于Python相对来讲通俗易懂的语言风格，语法简单且容易入门的特性，给很多数据科学领域的朋友，减轻了一部分学习编程语言的繁重。Pandas + NumPy + Matplotlib，这三者的结合基本可以胜任任意简单的数据分析和可视化的任务。复杂一点的可能还会需要SciPy的帮助。 本文目的这次，我打算用一篇长文来记录一下自己是如何利用Pandas进行数据分析的。网上有很多的Pandas入门教程，因此我这里并不打算针对所有Pandas的基础操作描述的那么清楚，还是希望更多的表达一些对于数据分析的想法和实现。 广义上，数据分析其实包含了从导入数据-&gt;清洗数据-&gt;分析数据-&gt;展示数据，这一从头到尾的流程。狭义上，数据分析指的就是中间分析数据这一块内容。本文按照广义上的数据分析的过程来一步步探讨。 接下来我们就正式开始本次数据分析之旅。 正文下面的这一段代码主要是包的调用和一些环境配置，Seaborn是也是一个plot包，可用来画出比Matplotlib更漂亮的图，它本身是基于Matplotlib设计的，对NumPy和Pandas都有很好的支持。这里我就不做过多解释了，对Seaborn有兴趣的朋友可以留言咨询或者自行探索。12345678910import pandas as pdfrom matplotlib import pyplot as pltimport seaborn as sns %matplotlib inline sns.set(rc=&#123;"figure.figsize": (10, 6.25)&#125;)sns.set_style("darkgrid")colors = ["windows blue", "amber", "faded green", "greyish", "dusty purple", "red violet", "marine", "jungle green", "chocolate brown", "dull pink", "reddish orange"]sns.set_palette(sns.xkcd_palette(colors)) 导入数据我这次用的数据是IGN上近20年来的各种平台的游戏，来源于这里。 1reviews = pd.read_csv("ign.csv") 数据读入之后，我们来看一下这里都有些什么内容。1reviews.head() Unnamed: 0 score_phrase title url platform score genre editors_choice release_year release_month release_day 0 Amazing LittleBigPlanet PS Vita /games/littlebigplanet-vita/vita-98907 PlayStation Vita 9.0 Platformer Y 2012 9 12 1 Amazing LittleBigPlanet PS Vita – Marvel Super Hero E… /games/littlebigplanet-ps-vita-marvel-super-he… PlayStation Vita 9.0 Platformer Y 2012 9 12 2 Great Splice: Tree of Life /games/splice/ipad-141070 iPad 8.5 Puzzle N 2012 9 12 3 Great NHL 13 /games/nhl-13/xbox-360-128182 Xbox 360 8.5 Sports N 2012 9 11 4 Great NHL 13 /games/nhl-13/ps3-128181 PlayStation 3 8.5 Sports N 2012 9 11 5 Amazing LittleBigPlanet PS Vita /games/littlebigplanet-vita/vita-98907 PlayStation Vita 9.0 Platformer Y 2012 9 12 6 Amazing LittleBigPlanet PS Vita – Marvel Super Hero E… /games/littlebigplanet-ps-vita-marvel-super-he… PlayStation Vita 9.0 Platformer Y 2012 9 12 7 Great Splice: Tree of Life /games/splice/ipad-141070 iPad 8.5 Puzzle N 2012 9 12 8 Great NHL 13 /games/nhl-13/xbox-360-128182 Xbox 360 8.5 Sports N 2012 9 11 9 Great NHL 13 /games/nhl-13/ps3-128181 PlayStation 3 8.5 Sports N 2012 9 11 这里先简单介绍一下每一列都代表什么吧： score_phrase – IGN用一个词来评价当前游戏，与得分直接相关； title – 游戏名称； url – 完整评论的地址； platform – 游戏平台（PS4, PC, Xbox, etc.）； score – 游戏的具体评分，从1.0到10.0； genre – 游戏分类； editors_choice – 是否为IGN编辑推荐的游戏，与评分有关系。 release_year – 游戏发布年份； release_month – 发布月份； release_day – 发布日期。 我们来看下总共多少个数据。1reviews.shape (18625, 11) 看来我们这次的数据里一共18625条数据，一共11列属性。 清洗数据源数据导入后一般来说是不能直接使用的，需要进行一定范围的数据清洗，不过本次的数据基本不需要清洗，收集这个数据的Eric Grinstein已经对数据进行了清洗工作。不过这里我们仍需要做一点简单的清洗工作，去除一些我们不需要的内容。 12reviews = reviews.iloc[:, 1:]reviews.head() score_phrase title url platform score genre editors_choice release_year release_month release_day 0 Amazing LittleBigPlanet PS Vita /games/littlebigplanet-vita/vita-98907 PlayStation Vita 9.0 Platformer Y 2012 9 12 1 Amazing LittleBigPlanet PS Vita – Marvel Super Hero E… /games/littlebigplanet-ps-vita-marvel-super-he… PlayStation Vita 9.0 Platformer Y 2012 9 12 2 Great Splice: Tree of Life /games/splice/ipad-141070 iPad 8.5 Puzzle N 2012 9 12 3 Great NHL 13 /games/nhl-13/xbox-360-128182 Xbox 360 8.5 Sports N 2012 9 11 4 Great NHL 13 /games/nhl-13/ps3-128181 PlayStation 3 8.5 Sports N 2012 9 11 分析数据数据清洗之后，其实就是分析过程的正式开始。在开始分析过程之前，先说点题外话。我本身对于游戏是很热爱的，从小到大，游戏机，掌机，PC，也拥有过不少的游戏平台。从和父母斗智斗勇中各种争取时间玩的红白机，小霸王学习机，到后来可以躲在被子里玩的GameBoy，但偶尔还得探出头担心父母进屋里发现自己的小秘密；再往后的世嘉，所以又不得不和父母软磨硬泡恳求游戏时间。直至家里第一台PC的出现，基本其他的游戏平台就很少碰了，除了后来的PSP，那是我从GameBoy之后时隔很多年再次拿起掌机玩游戏。要说起游戏，游戏平台，游戏的历史，真的说上三天三夜也说不完，其实这也是我为什么选择这么IGN的这个数据作为数据分析的数据来源。我也很想看看这20年来电子游戏产业的发展和趋势。 好了，咱们言归正传，就我个人而言，拿到这么多的数据之后，第一反应是：这么多的游戏，究竟是分布在了多少平台上呢？我亲身体验过的平台其实并不多，大概10个左右吧。那么这个数据集里究竟包含了多少平台呢？ 12all_platforms = reviews["platform"].unique()all_platforms array([&apos;PlayStation Vita&apos;, &apos;iPad&apos;, &apos;Xbox 360&apos;, &apos;PlayStation 3&apos;, &apos;Macintosh&apos;, &apos;PC&apos;, &apos;iPhone&apos;, &apos;Nintendo DS&apos;, &apos;Nintendo 3DS&apos;, &apos;Android&apos;, &apos;Wii&apos;, &apos;PlayStation 4&apos;, &apos;Wii U&apos;, &apos;Linux&apos;, &apos;PlayStation Portable&apos;, &apos;PlayStation&apos;, &apos;Nintendo 64&apos;, &apos;Saturn&apos;, &apos;Lynx&apos;, &apos;Game Boy&apos;, &apos;Game Boy Color&apos;, &apos;NeoGeo Pocket Color&apos;, &apos;Game.Com&apos;, &apos;Dreamcast&apos;, &apos;Dreamcast VMU&apos;, &apos;WonderSwan&apos;, &apos;Arcade&apos;, &apos;Nintendo 64DD&apos;, &apos;PlayStation 2&apos;, &apos;WonderSwan Color&apos;, &apos;Game Boy Advance&apos;, &apos;Xbox&apos;, &apos;GameCube&apos;, &apos;DVD / HD Video Game&apos;, &apos;Wireless&apos;, &apos;Pocket PC&apos;, &apos;N-Gage&apos;, &apos;NES&apos;, &apos;iPod&apos;, &apos;Genesis&apos;, &apos;TurboGrafx-16&apos;, &apos;Super NES&apos;, &apos;NeoGeo&apos;, &apos;Master System&apos;, &apos;Atari 5200&apos;, &apos;TurboGrafx-CD&apos;, &apos;Atari 2600&apos;, &apos;Sega 32X&apos;, &apos;Vectrex&apos;, &apos;Commodore 64/128&apos;, &apos;Sega CD&apos;, &apos;Nintendo DSi&apos;, &apos;Windows Phone&apos;, &apos;Web Games&apos;, &apos;Xbox One&apos;, &apos;Windows Surface&apos;, &apos;Ouya&apos;, &apos;New Nintendo 3DS&apos;, &apos;SteamOS&apos;], dtype=object) 这么多的平台……说实话，这里有很多我听都没听过，像Dreamcast，Atari 2600，Vectrex等等，看来这20年，游戏产业的发展还是很多元化的，至少从游戏平台上就可以看出端倪。 有了游戏平台的信息，自然而然地就会问，每个平台大概都出过多少游戏呢？1reviews["platform"].value_counts(dropna=False) PC 3370 PlayStation 2 1686 Xbox 360 1631 Wii 1366 PlayStation 3 1356 Nintendo DS 1045 PlayStation 952 Wireless 910 iPhone 842 Xbox 821 PlayStation Portable 633 Game Boy Advance 623 GameCube 509 Game Boy Color 356 Nintendo 64 302 Dreamcast 286 PlayStation 4 277 Nintendo DSi 254 Nintendo 3DS 225 Xbox One 208 PlayStation Vita 155 Wii U 114 iPad 99 Lynx 82 Macintosh 81 Genesis 58 NES 49 TurboGrafx-16 40 Android 39 Super NES 33 NeoGeo Pocket Color 31 N-Gage 30 Game Boy 22 iPod 17 Sega 32X 16 Windows Phone 14 Master System 13 Arcade 11 Linux 10 NeoGeo 10 Nintendo 64DD 7 Commodore 64/128 6 Saturn 6 Atari 2600 5 WonderSwan 4 TurboGrafx-CD 3 Game.Com 3 Atari 5200 2 New Nintendo 3DS 2 Vectrex 2 Pocket PC 1 WonderSwan Color 1 Ouya 1 Web Games 1 SteamOS 1 Dreamcast VMU 1 Windows Surface 1 DVD / HD Video Game 1 Sega CD 1 Name: platform, dtype: int64 从上面的统计来看，PC端无疑是最大的贡献者，这也可以理解，毕竟个人电脑从上个世纪末开始出现井喷，到后来虽然出货量开始下降，但一直都是人们学习生活娱乐中不可或缺的一部分，并且早期的个人电脑绝大部分都是以Windows为操作系统。不过让我没想到的是Dreamcast竟然还有286款游戏，看来是我孤陋寡闻了…… 下面来看看排名前十的平台都有哪些。 12platforms = reviews["platform"].value_counts()[:10].index.tolist()platforms [&apos;PC&apos;, &apos;PlayStation 2&apos;, &apos;Xbox 360&apos;, &apos;Wii&apos;, &apos;PlayStation 3&apos;, &apos;Nintendo DS&apos;, &apos;PlayStation&apos;, &apos;Wireless&apos;, &apos;iPhone&apos;, &apos;Xbox&apos;] 既然前十的平台我已经知道了，那么下面来看看每个平台的游戏质量如何，虽然PC端的游戏最多，但不一定好游戏占比就是最多的，对吧？ 想知道每个平台的游戏质量如何，我得先从所有的数据中将只属于前十的平台的游戏提取出来。这里我创建一个filter，用来筛选游戏平台。12fil = reviews["platform"] == platforms[0] # create a filterfil 0 False 1 False 2 False 3 False 4 False 5 False 6 False 7 True 8 False 9 True ... 18615 False 18616 True 18617 False 18618 True 18619 True 18620 False 18621 False 18622 False 18623 False 18624 True Name: platform, Length: 18625, dtype: bool 1234for platform in platforms[1:]: fil |= reviews["platform"] == platform filtered_reviews = reviews[fil] 下面是提取出来的所有数据：1filtered_reviews score_phrase title url platform score genre editors_choice release_year release_month release_day 3 Great NHL 13 /games/nhl-13/xbox-360-128182 Xbox 360 8.5 Sports N 2012 9 11 4 Great NHL 13 /games/nhl-13/ps3-128181 PlayStation 3 8.5 Sports N 2012 9 11 6 Awful Double Dragon: Neon /games/double-dragon-neon/xbox-360-131320 Xbox 360 3.0 Fighting N 2012 9 11 7 Amazing Guild Wars 2 /games/guild-wars-2/pc-896298 PC 9.0 RPG Y 2012 9 11 8 Awful Double Dragon: Neon /games/double-dragon-neon/ps3-131321 PlayStation 3 3.0 Fighting N 2012 9 11 9 Good Total War Battles: Shogun /games/total-war-battles-shogun/pc-142564 PC 7.0 Strategy N 2012 9 11 10 Good Tekken Tag Tournament 2 /games/tekken-tag-tournament-2/ps3-124584 PlayStation 3 7.5 Fighting N 2012 9 11 11 Good Tekken Tag Tournament 2 /games/tekken-tag-tournament-2/xbox-360-124581 Xbox 360 7.5 Fighting N 2012 9 11 12 Good Wild Blood /games/wild-blood/iphone-139363 iPhone 7.0 NaN N 2012 9 10 13 Amazing Mark of the Ninja /games/mark-of-the-ninja-135615/xbox-360-129276 Xbox 360 9.0 Action, Adventure Y 2012 9 7 14 Amazing Mark of the Ninja /games/mark-of-the-ninja-135615/pc-143761 PC 9.0 Action, Adventure Y 2012 9 7 16 Okay Home: A Unique Horror Adventure /games/home-a-unique-horror-adventure/pc-137135 PC 6.5 Adventure N 2012 9 6 17 Great Avengers Initiative /games/avengers-initiative/iphone-141579 iPhone 8.0 Action N 2012 9 5 18 Mediocre Way of the Samurai 4 /games/way-of-the-samurai-4/ps3-23516 PlayStation 3 5.5 Action, Adventure N 2012 9 3 19 Good JoJo’s Bizarre Adventure HD /games/jojos-bizarre-adventure/xbox-360-137717 Xbox 360 7.0 Fighting N 2012 9 3 20 Good JoJo’s Bizarre Adventure HD /games/jojos-bizarre-adventure/ps3-137896 PlayStation 3 7.0 Fighting N 2012 9 3 21 Good Mass Effect 3: Leviathan /games/mass-effect-3-leviathan/xbox-360-138918 Xbox 360 7.5 RPG N 2012 8 31 22 Good Mass Effect 3: Leviathan /games/mass-effect-3-leviathan/ps3-138915 PlayStation 3 7.5 RPG N 2012 8 31 23 Good Mass Effect 3: Leviathan /games/mass-effect-3-leviathan/pc-138919 PC 7.5 RPG N 2012 8 31 24 Amazing Dark Souls (Prepare to Die Edition) /games/dark-souls-prepare-to-die-edition/pc-13… PC 9.0 Action, RPG Y 2012 8 31 25 Good Symphony /games/symphony/pc-136470 PC 7.0 Shooter N 2012 8 30 27 Good Tom Clancy’s Ghost Recon Phantoms /games/tom-clancys-ghost-recon-online/pc-109114 PC 7.5 Shooter N 2012 8 29 28 Great Thirty Flights of Loving /games/thirty-flights-of-loving/pc-138374 PC 8.0 Adventure N 2012 8 29 29 Okay Legasista /games/legasista/ps3-127147 PlayStation 3 6.5 Action, RPG N 2012 8 28 31 Great World of Warcraft: Mists of Pandaria /games/world-of-warcraft-mists-of-pandaria/pc-… PC 8.7 RPG Y 2012 10 4 32 Bad Hell Yeah! Wrath of the Dead Rabbit /games/hell-yeah-wrath-of-the-dead-rabbit/ps3-… PlayStation 3 4.9 Platformer N 2012 10 4 33 Amazing Pokemon White Version 2 /games/pokemon-white-version-2/nds-129228 Nintendo DS 9.6 RPG Y 2012 10 3 34 Good War of the Roses /games/war-of-the-roses-140577/pc-115849 PC 7.3 Action N 2012 10 3 35 Amazing Pokemon Black Version 2 /games/pokemon-black-version-2/nds-129224 Nintendo DS 9.6 RPG Y 2012 10 3 36 Okay Drakerider /games/drakerider/iphone-135745 iPhone 6.5 RPG N 2012 10 3 … … … … … … … … … … … 18546 Great Devil Daggers /games/devil-daggers/pc-20049771 PC 8.5 Shooter N 2016 2 27 18547 Good Superhot /games/superhot/pc-20018899 PC 7.5 Action N 2016 2 25 18549 Good Battleborn /games/battleborn/pc-20021225 PC 7.1 Shooter N 2016 5 6 18554 Good The Park /games/the-park/pc-20042102 PC 7.0 Adventure N 2016 5 4 18555 Great Hitman: Episode 2 /games/hitman-episode-2/pc-20051629 PC 8.5 Shooter N 2016 4 29 18557 Amazing Hearts of Iron IV /games/hearts-of-iron-iv/pc-20012080 PC 9.0 Strategy Y 2016 6 6 18559 Okay Dangerous Golf /games/dangerous-golf/pc-20048436 PC 6.0 Sports, Action N 2016 6 3 18567 Great Offworld Trading Company /games/offworld-trading-company/pc-20018639 PC 8.0 Strategy N 2016 4 28 18568 Okay The Walking Dead: Michonne – Episode 3: What … /games/the-walking-dead-michonne-episode-3/pc-… PC 6.3 Adventure N 2016 4 27 18570 Good Battlefleet Gothic: Armada /games/battlefleet-gothic-armada/pc-20030300 PC 7.1 Strategy N 2016 4 22 18572 Amazing Overwatch /games/overwatch/pc-20027413 PC 9.4 Shooter Y 2016 5 28 18575 Good Fallout 4: Nuka World /games/fallout-4-nuka-world/pc-20054761 PC 7.9 RPG N 2016 8 30 18578 Good Master of Orion /games/master-of-orion-wargaming/pc-20038452 PC 7.1 Strategy N 2016 8 26 18580 Great Quadrilateral Cowboy /games/quadrilateral-cowboy/pc-159788 PC 8.5 Puzzle N 2016 7 28 18581 Great Fallout 4: Vault-Tec Workshop /games/fallout-4-vault-tec-workshop/pc-20054769 PC 8.2 RPG N 2016 7 27 18583 Great Kentucky Route Zero: Act 4 /games/kentucky-route-zero-act-4/pc-20046280 PC 8.5 Adventure N 2016 7 22 18586 Great F1 2016 /games/f1-2016/pc-20054151 PC 8.8 Racing N 2016 8 24 18589 Amazing Deus Ex: Mankind Divided /games/deus-ex-mankind-divided/pc-20013794 PC 9.2 Action, RPG Y 2016 8 19 18595 Bad Ghostbusters /games/ghostbusters-the-movie/pc-20052317 PC 4.4 Action N 2016 7 16 18596 Okay Necropolis /games/necropolis/pc-20030346 PC 6.5 Action, Adventure N 2016 7 14 18598 Okay Furi /games/furi/pc-20044439 PC 6.8 Action N 2016 7 13 18600 Good Hitman: Episode 4 /games/hitman-episode-4/pc-20051637 PC 7.4 Shooter N 2016 8 19 18603 Good Grow Up /games/grow-up/pc-20054824 PC 7.8 Platformer N 2016 8 18 18606 Okay Starcraft II: Nova Covert Ops – Mission Pack 2 /games/starcraft-ii-nova-covert-ops-mission-pa… PC 6.4 Strategy N 2016 8 4 18607 Good Pokemon Go /games/pokemon-go/iphone-20042699 iPhone 7.0 Battle N 2016 7 13 18613 Great XCOM 2: Shen’s Last Gift /games/xcom-2-shens-last-gift/pc-20055520 PC 8.0 Strategy N 2016 7 1 18616 Good Batman: The Telltale Series – Episode 1: Real… /games/batman-the-telltale-series-episode-1-re… PC 7.5 Adventure N 2016 8 2 18618 Amazing Starbound /games/starbound-2016/pc-128879 PC 9.1 Action Y 2016 7 28 18619 Good Human Fall Flat /games/human-fall-flat/pc-20051928 PC 7.9 Puzzle, Action N 2016 7 28 18624 Masterpiece Inside /games/inside-playdead/pc-20055740 PC 10.0 Adventure Y 2016 6 28 13979 rows × 10 columns 展示数据现在已经有了前十平台的数据，需要思考的就是如何来呈现每个平台的游戏质量呢？当然可以用每个平台的score的平均值来对比，但未免有点单薄了。数据属性中有一列是score_phrase，用一个单词来形容当前游戏的好坏，与score直接挂钩，用这个来展示应该会更容易理解和分析。 这里可以用Matplotlib.pyplot的bar来画，也可以用Seaborn中的countplot，后者使用起来更容易方便。1sns.countplot(x="platform", hue="score_phrase", data=filtered_reviews, palette=sns.xkcd_palette(colors)); 展示的结果如上图所示，我们可以看到PC平台下，Great和Good这两栏下的游戏数量基本就占了大半，但我并不能说PC端的游戏质量就比其他平台高出一筹，因为我们依然无法判断每个平台下优秀的作品占比如何。这幅图只能直观地告诉我们每个平台下，所有分数的一个分布状况。 所以，下面的工作，我要继续细化一下数据分析和展示的部分。 进一步分析与展示数据因为原先划分的score_phrase太多了，我决定将它们重新划为三个部分：好于Good的，差于Okay的，剩下的就是中间部分。我的这个标准可能比较严格，在我看来，评分8.0以上的才算的上是优秀的作品，也就是高于Good的；至于那些评分低于6.0的，也就是还不到Okay的，算作差劲也不算失礼吧。 12345678910111213141516all_score_phrases = set(reviews["score_phrase"].unique())bt_good = set(['Great', 'Amazing', 'Masterpiece'])average = set(['Good', 'Okay'])wt_okay = all_score_phrases - bt_good - average def category_score_phrase(value): if value in bt_good: return "Better than Good" elif value in wt_okay: return "Worse than Okay" else: return "Average" sizes = filtered_reviews["score_phrase"].apply(category_score_phrase).value_counts()explode = (0, 0.1, 0)plt.pie(sizes, labels=sizes.index, explode=explode, autopct='%1.2f%%', shadow=True, startangle=90); 这里我先用饼图来展示一下前十的平台，整体的游戏质量分布情况。 这里，我创建了一个新列，叫score_phrase_new，为了区别原有的score_phrase。 12filtered_reviews["score_phrase_new"] = filtered_reviews["score_phrase"].apply(category_score_phrase)filtered_reviews.head() score_phrase title url platform score genre editors_choice release_year release_month release_day score_phrase_new 3 Great NHL 13 /games/nhl-13/xbox-360-128182 Xbox 360 8.5 Sports N 2012 9 11 Better than Good 4 Great NHL 13 /games/nhl-13/ps3-128181 PlayStation 3 8.5 Sports N 2012 9 11 Better than Good 6 Awful Double Dragon: Neon /games/double-dragon-neon/xbox-360-131320 Xbox 360 3.0 Fighting N 2012 9 11 Worse than Okay 7 Amazing Guild Wars 2 /games/guild-wars-2/pc-896298 PC 9.0 RPG Y 2012 9 11 Better than Good 8 Awful Double Dragon: Neon /games/double-dragon-neon/ps3-131321 PlayStation 3 3.0 Fighting N 2012 9 11 Worse than Okay 先来用数字直观地看一下每个平台下，每个评分阶段的数量。1filtered_reviews.groupby(["platform", "score_phrase_new"]).count() score_phrase title url score genre editors_choice release_year release_month release_day platform score_phrase_new Nintendo DS Average 462 462 462 462 462 462 462 462 462 Better than Good 207 207 207 207 207 207 207 207 207 Worse than Okay 376 376 376 376 375 376 376 376 376 PC Average 1394 1394 1394 1394 1393 1394 1394 1394 1394 Better than Good 1323 1323 1323 1323 1322 1323 1323 1323 1323 Worse than Okay 653 653 653 653 652 653 653 653 653 PlayStation Average 362 362 362 362 362 362 362 362 362 Better than Good 313 313 313 313 313 313 313 313 313 Worse than Okay 277 277 277 277 277 277 277 277 277 PlayStation 2 Average 716 716 716 716 716 716 716 716 716 Better than Good 542 542 542 542 542 542 542 542 542 Worse than Okay 428 428 428 428 426 428 428 428 428 PlayStation 3 Average 516 516 516 516 515 516 516 516 516 Better than Good 569 569 569 569 569 569 569 569 569 Worse than Okay 271 271 271 271 271 271 271 271 271 Wii Average 551 551 551 551 547 551 551 551 551 Better than Good 321 321 321 321 321 321 321 321 321 Worse than Okay 494 494 494 494 494 494 494 494 494 Wireless Average 473 473 473 473 471 473 473 473 473 Better than Good 308 308 308 308 306 308 308 308 308 Worse than Okay 129 129 129 129 129 129 129 129 129 Xbox Average 307 307 307 307 307 307 307 307 307 Better than Good 354 354 354 354 354 354 354 354 354 Worse than Okay 160 160 160 160 160 160 160 160 160 Xbox 360 Average 631 631 631 631 631 631 631 631 631 Better than Good 646 646 646 646 646 646 646 646 646 Worse than Okay 354 354 354 354 354 354 354 354 354 iPhone Average 412 412 412 412 405 412 412 412 412 Better than Good 321 321 321 321 315 321 321 321 321 Worse than Okay 109 109 109 109 108 109 109 109 109 事实上，上面的表格大部分内容也用不上，我们需要的其实就三列：游戏平台，评分阶段和数量。因此我就压缩一下原表格，让它变成下面的样子。123count_df = filtered_reviews.groupby(["platform", "score_phrase_new"]).count().reset_index().iloc[:, :3]count_df.rename(columns=&#123;"score_phrase": "count"&#125;, inplace=True)count_df platform score_phrase_new count 0 Nintendo DS Average 462 1 Nintendo DS Better than Good 207 2 Nintendo DS Worse than Okay 376 3 PC Average 1394 4 PC Better than Good 1323 5 PC Worse than Okay 653 6 PlayStation Average 362 7 PlayStation Better than Good 313 8 PlayStation Worse than Okay 277 9 PlayStation 2 Average 716 10 PlayStation 2 Better than Good 542 11 PlayStation 2 Worse than Okay 428 12 PlayStation 3 Average 516 13 PlayStation 3 Better than Good 569 14 PlayStation 3 Worse than Okay 271 15 Wii Average 551 16 Wii Better than Good 321 17 Wii Worse than Okay 494 18 Wireless Average 473 19 Wireless Better than Good 308 20 Wireless Worse than Okay 129 21 Xbox Average 307 22 Xbox Better than Good 354 23 Xbox Worse than Okay 160 24 Xbox 360 Average 631 25 Xbox 360 Better than Good 646 26 Xbox 360 Worse than Okay 354 27 iPhone Average 412 28 iPhone Better than Good 321 29 iPhone Worse than Okay 109 数据拿到手了，下面又该是用图形展示数据的时候。这次我们来看一下每一个评分阶段对于各自游戏平台占比究竟是多少。 1234567bar_width = 1bar_left = [i for i in range(len(count_df) // 3)]tick_pos = [i + (bar_width / 2) for i in bar_left]totals = [i + j + k for i, j, k in zip(count_df["count"][::3], count_df["count"][1::3], count_df["count"][2::3])]ave_perc = [i / j * 100 for i, j in zip(count_df["count"][::3], totals)]bt_good_perc = [i / j * 100 for i, j in zip(count_df["count"][1::3], totals)]wt_okay_perc = [i / j * 100 for i, j in zip(count_df["count"][2::3], totals)] 12345678910111213141516171819202122232425262728293031f, ax = plt.subplots(1) ax.bar(bar_left, wt_okay_perc, label="Worst than Okay", alpha=0.9, width=bar_width, edgecolor="white") ax.bar(bar_left, ave_perc, bottom=wt_okay_perc, label="Average", alpha=0.9, width=bar_width, edgecolor="white") ax.bar(bar_left, bt_good_perc, bottom=[i+j for i, j in zip(wt_okay_perc, ave_perc)], label="Better than Good", alpha=0.9, width=bar_width, edgecolor="white") plt.xticks(tick_pos, set(count_df["platform"]))ax.set_ylabel("Percentage")ax.set_xlabel("")plt.legend(bbox_to_anchor=(1., 1.))plt.setp(plt.gca().get_xticklabels(), rotation=45, horizontalalignment="right")plt.show() 我并没有直接把具体百分比的数值标记在上面，不过通过直观的图形依然可以看到一些信息。从图中可以看出来，相对来说，PlayStation，PlayStation2，Wii，PC和Nintendo DS的游戏质量都是很不错的，高质量游戏占比高，且低质量游戏占比低。PlayStation3虽然低质量游戏占比很小，但是高品质游戏也不算很多。iPhone和Xbox的表现算是最差的了，低质量游戏占比分属最高的一二，高品质游戏也是最低的两个平台。其实iPhone是这样的倒是不意外了，因为毕竟iPhone平台的起点相对于其他的平台要低很多，基本上三五个人，甚至一个人做出的游戏都有，这样很难保证游戏兼顾趣味性和剧情或者其他方面。在后期维护上面肯定也要比大公司开发的游戏差了很多。遗憾的是Xbox竟然也有如此差劲的表现，着实令我难以理解。 总结至此，我打算分析的内容就呈现完了，这就是我个人拿到数据之后一个简单的想法，然后试着去将这个想法用数据分析的方法展现出来，供自己去理解。后面我还会对这个数据集进一步的分析，比如去探讨一下年份和分数的关系，游戏类别和分数的关系。希望这篇文章可以起到抛砖引玉的作用，能让各位看完之后对于如何开始分析一份数据有自己的想法。 各位看官对于本文有任何不明白的地方，欢迎提问，也欢迎指正和建议。 Related Links Pandas Tutorial: Data analysis with Python: Part 1 Pandas Tutorial: Data analysis with Python: Part 2 Stacked Percentage Bar Plot In MatPlotLib]]></content>
      <categories>
        <category>Data analysis</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>Pandas</tag>
        <tag>Data analysis</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[用Python在Hadoop上跑MapReduce]]></title>
    <url>%2F2017%2F09%2F29%2FMap-reduce-works-perfectly-with-Python%2F</url>
    <content type="text"><![CDATA[本文目的这篇文章主要会给大家介绍一下如何将Python和Hadoop结合起来工作。有接触过MapReduce的朋友都知道，Hadoop的运行环境主要是Java，一般介绍Hadoop和MapReduce的教程和书籍也都是基于Java的。因为我个人对Java并不太感冒，一直以来钟情于Python的简洁实用理念，同时又对MapReduce有兴趣，因此萌生了Python的MapReduce结合的想法。本文也是我经过Google学习他人教程，以及自己实际练习得出来的一些心得，在此分享给各位。 环境搭建首先，你需要有个Hadoop的运行环境，还有Python运行环境。本文主要目的不在分享安装环境，因此有从零开始的朋友，可以先去百度或者Google上搜一下相关教程。下面分享几个相关的教程： Hadoop: Setting up a Single Node Cluster Hadoop安装教程_单机/伪分布式配置_Hadoop2.6.0/Ubuntu14.04 使用Docker在本地搭建Hadoop分布式集群 MapReduce in Python下面我就来看Python里如何实现mapper和reducer。 mapper.pymapper要做的工作就是从stdin里读取数据，然后分割成&lt;key, value&gt;的pair。这里以最基础的word count为例，key就是指文章中拆出来的词，value就是指每个词的个数。mapper是不会将相同的词的个数进行统计加和的，那是reducer的工作，因此mapper的输出就是由很多行&lt;key&gt; 1组成，下面会看到程序运行实际的结果。 mapper.py1234567891011#!/usr/bin/env python3 import sys # 从stdin中获取输出信息for line in sys.stdin: words = line.strip().split(" ") for word in words: # output format: word'\t'1 print("&#123;0&#125;\t&#123;1&#125;".format(word, 1)) reducer.py上面提到了，mapper只进行词汇分割，计数为1的工作，那么reducer就是用来将相同词汇的出现次数进行统计加和的工作。同mapper一样，reducer也是从stdin中获取输入，然后将结果输出到stdout。 reducer.py12345678910111213141516171819202122#!/usr/bin/env python3 import sys last_word = Nonetotal_count = 0 for line in sys.stdin: word, count = line.strip().split("\t") count = int(count) # 如果当前的word不等于上一个，说明开始新的了 if last_word and last_word != word: # 这里输出结果，相当于把结果写进stdout print("&#123;0&#125;\t&#123;1&#125;".format(last_word, total_count)) total_count = 0 last_word = word total_count += count # 不要忘记最后一个word的输出if last_word: print("&#123;0&#125;\t&#123;1&#125;".format(last_word, total_count)) 本地测试我们先来本地测试一下正确性。12345678YuanMBP:src Vergil$ echo "东 南 西 北 中 发 白" | demo/mapper.py 东 1南 1西 1北 1中 1发 1白 1 再来看一下reducer：12345678YuanMBP:src Vergil$ echo "东 南 西 北 东 中 东 发 东 白" | demo/mapper.py | sort | demo/reducer.py 东 4中 1北 1南 1发 1白 1西 1 请注意，这里我在运行mapper和reducer之间加入了一个sort，这是必须的，了解map-reduce工作原理的朋友应该都明白这里为什么有一个sort，如果不加的话，我们的东风杠就识别不了啦。我们在写mapper和reducer的时候是不需要关注它排序的问题，因为Hadoop中的map-reduce会自动进行排序。1234567891011YuanMBP:src Vergil$ echo "东 南 西 北 东 中 东 发 东 白" | demo/mapper.py | demo/reducer.py 东 1南 1西 1北 1东 1中 1东 1发 1东 1白 1 在Hadoop上跑程序准备测试数据我的运行环境是在Docker上搭建的，首先我们需要先把用来测试的文章放到HDFS里12345root@hadoop-master:~/src/demo# hdfs dfs -put words1.txt /inputroot@hadoop-master:~/src/demo# hdfs dfs -ls /inputFound 1 item-rw-r--r-- 2 root supergroup 127 2017-09-30 03:57 /input/words1.txtroot@hadoop-master:~/src/demo# words1.txt1234Let me write down something trivialsomething that is not importantsomething that looks like bullshityes that is what I want 我就随便写了几句放在words1.txt里，看看运行结果是否正确。 运行MapReduce用过Java版Hadoop的朋友，应该还有印象如何编译运行吧，其实就和运行Java程序的过程很像。但这里用Python来执行，就稍微有些不太一样了。首先我们需要用到一个hadoop-streaming-2.x.x.jar这样的一个工具，这里xx代表版本号。它的具体解释可以参考Hadoop官方给的Document，我这里就做个简单的介绍。Hadoop Streaming是Hadoop提供的一个工具，可以让你以任意的可执行程序或脚本，来创建和运行MapReduce，这里官网给了一个简单的例子：12345$HADOOP_HOME/bin/hadoop jar $HADOOP_HOME/hadoop-streaming.jar \ -input myInputDirs \ -output myOutputDir \ -mapper /bin/cat \ -reducer /bin/wc 现在，我们来用Hadoop Streaming来运行自己的程序。1234567root@hadoop-master:~/src/demo# hadoop jar ../hadoop-streaming-2.7.2.jar \&gt; -input /input \&gt; -output /output \&gt; -mapper mapper.py \&gt; -reducer reducer.py \&gt; -file mapper.py \&gt; -file reducer.py 这里有两点需要注意： 后面的两个-file是必须要加的，否则程序无法顺利运行； mapper.py和reducer.py要提前记得赋予它们可执行的属性。 接下来，我们来验收一下程序的结果：1234567891011121314151617181920212223root@hadoop-master:~/src/demo# hdfs dfs -ls /outputFound 2 items-rw-r--r-- 2 root supergroup 0 2017-09-30 04:31 /output/_SUCCESS-rw-r--r-- 2 root supergroup 128 2017-09-30 04:31 /output/part-00000root@hadoop-master:~/src/demo# hdfs dfs -cat /output/part*I 1Let 1bullshit 1down 1important 1is 2like 1looks 1me 1not 1something 3that 3trivial 1want 1what 1write 1yes 1root@hadoop-master:~/src/demo# 试一下多文件看看有没有问题，我将words1.txt复制出一模一样的两份，也就是现在有三份相同的输入文件，再来跑一遍试试：123456root@hadoop-master:~/src/demo# hdfs dfs -rm -r /output17/09/30 04:48:37 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.Deleted /outputroot@hadoop-master:~/src/demo# ./run_script.sh /input /output mapper.py reducer.py Running python in Hadoop by hadoop streaming... 这里我自己做了run_script.sh这样子一个shell script，用来缩短执行命令的长度，不然每次都要输入那么长真的好麻烦……123456789101112131415161718root@hadoop-master:~/src/demo# hdfs dfs -cat /output/part*I 3Let 3bullshit 3down 3important 3is 6like 3looks 3me 3not 3something 9that 9trivial 3want 3what 3write 3yes 3 看上去没有任何问题。 后记更多的关于Hadoop Streaming的内容，还希望大家去官网文档中查阅。例如像分配map和reduce的数量，设置partitioner，这样的参数都可以通过Hadoop Streaming来调整，还是很有意思的。 在下一篇关于MapReduce的文章中我会介绍一个相比较于word count复杂一点的例子，依然是用Python和Hadoop的结合。 Related Links Writing an hadoop mapreduce program in python Hadoop Streaming Documentation]]></content>
      <categories>
        <category>Big Data</category>
      </categories>
      <tags>
        <tag>Hadoop</tag>
        <tag>Python</tag>
        <tag>Docker</tag>
        <tag>MapReduce</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Migrated from Heroku to Netlify]]></title>
    <url>%2F2017%2F09%2F28%2FNetlify%2F</url>
    <content type="text"><![CDATA[之前学习ruby on rails的时候，开始接触的Heroku。这次部署个人博客，想着Heroku应该是个不错的选择吧，但试用了几天的Heroku之后，我决定还是转到其他地方部署我的blog，为什么呢？原因有二： 免费的Heroku App，如果搁置时间长了一直无人访问的话，再次访问是需要一定时间等待激活的，就像是电脑睡眠了等着叫醒一样。付费的话太贵，对于只是搭载个人博客而言不太划算； Heroku服务器是在美国和欧洲，而且一个App只能存在在某一个区域，不可变。所以对于国内的朋友，登录我的博客有时会有不小的延迟，虽然对于一个静态网站，本来也无需很快的响应速度，但配合第一条，这个时间有时真的让人无语…… 那之所以选择Netlify也是有两个原因： 不存在Heroku的第一个问题； 静态内容部署采用的是global CDN方式，这样用户登录博客的时候会根据用户所在地选取最近的节点获取信息。 除了这两点以外，Netlify的设置界面也做得十分简洁易用，相比Heroku而言，甚至连部署都更加简单，除了利用git部署，还可以直接把生成好的静态网页文件夹拖动到一个部署框内]]></content>
      <categories>
        <category>general</category>
      </categories>
      <tags>
        <tag>Netlify</tag>
        <tag>migrate</tag>
        <tag>Heroku</tag>
        <tag>static site</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hello World]]></title>
    <url>%2F2017%2F09%2F25%2Fhello-world%2F</url>
    <content type="text"><![CDATA[前言终于将个人博客搭建起来了……虽然听说Hexo搭建博客十分容易，但对于前端完全不了解的我，颇费了一番功夫。 Hexo初始化这个博客目录时，自带了一个以“Hello World”为标题的文章，考虑到“Hello World”是作为学习编程代码第一课，作为一个标准码农的我就决定沿袭这个传统，从此开始自己的博客之旅。 Hello Hexo Hello World]]></content>
      <categories>
        <category>general</category>
      </categories>
      <tags>
        <tag>First</tag>
        <tag>Blog</tag>
      </tags>
  </entry>
</search>
