<?xml version="1.0" encoding="utf-8"?>
<search> 
  
    
    <entry>
      <title><![CDATA[用Python在Hadoop上跑MapReduce]]></title>
      <url>/2017/09/29/Map-reduce-works-perfectly-with-Python/</url>
      <content type="html"><![CDATA[<h1 id="本文目的"><a href="#本文目的" class="headerlink" title="本文目的"></a>本文目的</h1><p>这篇文章主要会给大家介绍一下如何将Python和Hadoop结合起来工作。有接触过MapReduce的朋友都知道，Hadoop的运行环境主要是Java，一般介绍Hadoop和MapReduce的教程和书籍也都是基于Java的。因为我个人对Java并不太感冒，一直以来钟情于Python的简洁实用理念，同时又对MapReduce有兴趣，因此萌生了Python的MapReduce结合的想法。本文也是我经过Google学习他人教程，以及自己实际练习得出来的一些心得，在此分享给各位。</p>
<hr>
<h1 id="环境搭建"><a href="#环境搭建" class="headerlink" title="环境搭建"></a>环境搭建</h1><p>首先，你需要有个Hadoop的运行环境，还有Python运行环境。本文主要目的不在分享安装环境，因此有从零开始的朋友，可以先去百度或者Google上搜一下相关教程。下面分享几个相关的教程：</p>
<ul>
<li><a href="http://hadoop.apache.org/docs/stable/hadoop-project-dist/hadoop-common/SingleCluster.html" target="_blank" rel="external">Hadoop: Setting up a Single Node Cluster</a></li>
<li><a href="http://www.powerxing.com/install-hadoop/" target="_blank" rel="external">Hadoop安装教程_单机/伪分布式配置_Hadoop2.6.0/Ubuntu14.04</a></li>
<li><a href="http://www.tashan10.com/yong-dockerda-jian-hadoopwei-fen-bu-shi-ji-qun/" target="_blank" rel="external">使用Docker在本地搭建Hadoop分布式集群</a></li>
</ul>
<hr>
<h1 id="MapReduce-in-Python"><a href="#MapReduce-in-Python" class="headerlink" title="MapReduce in Python"></a>MapReduce in Python</h1><p>下面我就来看Python里如何实现mapper和reducer。</p>
<h2 id="mapper-py"><a href="#mapper-py" class="headerlink" title="mapper.py"></a>mapper.py</h2><p>mapper要做的工作就是从<code>stdin</code>里读取数据，然后分割成<code>&lt;key, value&gt;</code>的pair。这里以最基础的word count为例，<code>key</code>就是指文章中拆出来的词，<code>value</code>就是指每个词的个数。mapper是不会将相同的词的个数进行统计加和的，那是reducer的工作，因此mapper的输出就是由很多行<code>&lt;key&gt; 1</code>组成，下面会看到程序运行实际的结果。</p>
<p><code>mapper.py</code><br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div></pre></td><td class="code"><pre><div class="line"><span class="comment">#!/usr/bin/env python3</span></div><div class="line"></div><div class="line"><span class="keyword">import</span> sys</div><div class="line"></div><div class="line"><span class="comment"># 从stdin中获取输出信息</span></div><div class="line"><span class="keyword">for</span> line <span class="keyword">in</span> sys.stdin:</div><div class="line">    words = line.strip().split(<span class="string">" "</span>)</div><div class="line">    <span class="keyword">for</span> word <span class="keyword">in</span> words:</div><div class="line"></div><div class="line">        <span class="comment"># output format: word'\t'1</span></div><div class="line">        print(<span class="string">"&#123;0&#125;\t&#123;1&#125;"</span>.format(word, <span class="number">1</span>))</div></pre></td></tr></table></figure></p>
<hr>
<h2 id="reducer-py"><a href="#reducer-py" class="headerlink" title="reducer.py"></a>reducer.py</h2><p>上面提到了，mapper只进行词汇分割，计数为1的工作，那么reducer就是用来将相同词汇的出现次数进行统计加和的工作。同mapper一样，reducer也是从<code>stdin</code>中获取输入，然后将结果输出到<code>stdout</code>。</p>
<p><code>reducer.py</code><br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div></pre></td><td class="code"><pre><div class="line"><span class="comment">#!/usr/bin/env python3</span></div><div class="line"></div><div class="line"><span class="keyword">import</span> sys</div><div class="line"></div><div class="line">last_word = <span class="keyword">None</span></div><div class="line">total_count = <span class="number">0</span></div><div class="line"></div><div class="line"><span class="keyword">for</span> line <span class="keyword">in</span> sys.stdin:</div><div class="line">    word, count = line.strip().split(<span class="string">"\t"</span>)</div><div class="line">    count = int(count)</div><div class="line">    </div><div class="line">    <span class="comment"># 如果当前的word不等于上一个，说明开始新的了</span></div><div class="line">    <span class="keyword">if</span> last_word <span class="keyword">and</span> last_word != word:</div><div class="line">        <span class="comment"># 这里输出结果，相当于把结果写进stdout</span></div><div class="line">        print(<span class="string">"&#123;0&#125;\t&#123;1&#125;"</span>.format(last_word, total_count))</div><div class="line">        total_count = <span class="number">0</span></div><div class="line">    last_word = word</div><div class="line">    total_count += count</div><div class="line"></div><div class="line"><span class="comment"># 不要忘记最后一个word的输出</span></div><div class="line"><span class="keyword">if</span> last_word:</div><div class="line">    print(<span class="string">"&#123;0&#125;\t&#123;1&#125;"</span>.format(last_word, total_count))</div></pre></td></tr></table></figure></p>
<hr>
<h1 id="本地测试"><a href="#本地测试" class="headerlink" title="本地测试"></a>本地测试</h1><p>我们先来本地测试一下正确性。<br><figure class="highlight console"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line">YuanMBP:src Vergil$ echo "东 南 西 北 中 发 白" | demo/mapper.py </div><div class="line">东   1</div><div class="line">南   1</div><div class="line">西   1</div><div class="line">北   1</div><div class="line">中   1</div><div class="line">发   1</div><div class="line">白   1</div></pre></td></tr></table></figure></p>
<p>再来看一下reducer：<br><figure class="highlight console"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line">YuanMBP:src Vergil$ echo "东 南 西 北 东 中 东 发 东 白" | demo/mapper.py | sort | demo/reducer.py </div><div class="line">东   4</div><div class="line">中   1</div><div class="line">北   1</div><div class="line">南   1</div><div class="line">发   1</div><div class="line">白   1</div><div class="line">西   1</div></pre></td></tr></table></figure></p>
<p>请注意，这里我在运行mapper和reducer之间加入了一个sort，这是必须的，了解map-reduce工作原理的朋友应该都明白这里为什么有一个sort，如果不加的话，我们的东风杠就识别不了啦。我们在写mapper和reducer的时候是不需要关注它排序的问题，因为Hadoop中的map-reduce会自动进行排序。<br><figure class="highlight console"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div></pre></td><td class="code"><pre><div class="line">YuanMBP:src Vergil$ echo "东 南 西 北 东 中 东 发 东 白" | demo/mapper.py | demo/reducer.py </div><div class="line">东   1</div><div class="line">南   1</div><div class="line">西   1</div><div class="line">北   1</div><div class="line">东   1</div><div class="line">中   1</div><div class="line">东   1</div><div class="line">发   1</div><div class="line">东   1</div><div class="line">白   1</div></pre></td></tr></table></figure></p>
<hr>
<h1 id="在Hadoop上跑程序"><a href="#在Hadoop上跑程序" class="headerlink" title="在Hadoop上跑程序"></a>在Hadoop上跑程序</h1><h2 id="准备测试数据"><a href="#准备测试数据" class="headerlink" title="准备测试数据"></a>准备测试数据</h2><p>我的运行环境是在Docker上搭建的，首先我们需要先把用来测试的文章放到HDFS里<br><figure class="highlight console"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">root@hadoop-master:~/src/demo# hdfs dfs -put words1.txt /input</div><div class="line">root@hadoop-master:~/src/demo# hdfs dfs -ls /input</div><div class="line">Found 1 item</div><div class="line">-rw-r--r--   2 root supergroup        127 2017-09-30 03:57 /input/words1.txt</div><div class="line">root@hadoop-master:~/src/demo#</div></pre></td></tr></table></figure></p>
<p><code>words1.txt</code><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">Let me write down something trivial</div><div class="line">something that is not important</div><div class="line">something that looks like bullshit</div><div class="line">yes that is what I want</div></pre></td></tr></table></figure></p>
<p>我就随便写了几句放在<code>words1.txt</code>里，看看运行结果是否正确。</p>
<hr>
<h2 id="运行MapReduce"><a href="#运行MapReduce" class="headerlink" title="运行MapReduce"></a>运行MapReduce</h2><p>用过Java版Hadoop的朋友，应该还有印象如何编译运行吧，其实就和运行Java程序的过程很像。但这里用Python来执行，就稍微有些不太一样了。首先我们需要用到一个<code>hadoop-streaming-2.x.x.jar</code>这样的一个工具，这里xx代表版本号。它的具体解释可以参考Hadoop官方给的<a href="http://hadoop.apache.org/docs/r1.2.1/streaming.html#Hadoop+Streaming" target="_blank" rel="external">Document</a>，我这里就做个简单的介绍。<strong>Hadoop Streaming</strong>是Hadoop提供的一个工具，可以让你以任意的可执行程序或脚本，来创建和运行MapReduce，这里官网给了一个简单的例子：<br><figure class="highlight console"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line"><span class="meta">$</span>HADOOP_HOME/bin/hadoop  jar $HADOOP_HOME/hadoop-streaming.jar \</div><div class="line">    -input myInputDirs \</div><div class="line">    -output myOutputDir \</div><div class="line">    -mapper /bin/cat \</div><div class="line">    -reducer /bin/wc</div></pre></td></tr></table></figure></p>
<hr>
<p>现在，我们来用Hadoop Streaming来运行自己的程序。<br><figure class="highlight console"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line">root@hadoop-master:~/src/demo# hadoop jar ../hadoop-streaming-2.7.2.jar \</div><div class="line"><span class="meta">&gt;</span> -input /input \</div><div class="line"><span class="meta">&gt;</span> -output /output \</div><div class="line"><span class="meta">&gt;</span> -mapper mapper.py \</div><div class="line"><span class="meta">&gt;</span> -reducer reducer.py \</div><div class="line"><span class="meta">&gt;</span> -file mapper.py \</div><div class="line"><span class="meta">&gt;</span> -file reducer.py</div></pre></td></tr></table></figure></p>
<hr>
<p>这里有两点需要注意：</p>
<ul>
<li>后面的两个<code>-file</code>是必须要加的，否则程序无法顺利运行；</li>
<li><code>mapper.py</code>和<code>reducer.py</code>要提前记得赋予它们可执行的属性。</li>
</ul>
<p>接下来，我们来验收一下程序的结果：<br><figure class="highlight console"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div></pre></td><td class="code"><pre><div class="line">root@hadoop-master:~/src/demo# hdfs dfs -ls /output</div><div class="line">Found 2 items</div><div class="line">-rw-r--r--   2 root supergroup          0 2017-09-30 04:31 /output/_SUCCESS</div><div class="line">-rw-r--r--   2 root supergroup        128 2017-09-30 04:31 /output/part-00000</div><div class="line">root@hadoop-master:~/src/demo# hdfs dfs -cat /output/part*</div><div class="line">I   1</div><div class="line">Let 1</div><div class="line">bullshit    1</div><div class="line">down    1</div><div class="line">important   1</div><div class="line">is  2</div><div class="line">like    1</div><div class="line">looks   1</div><div class="line">me  1</div><div class="line">not 1</div><div class="line">something   3</div><div class="line">that    3</div><div class="line">trivial 1</div><div class="line">want    1</div><div class="line">what    1</div><div class="line">write   1</div><div class="line">yes 1</div><div class="line">root@hadoop-master:~/src/demo#</div></pre></td></tr></table></figure></p>
<hr>
<p>试一下多文件看看有没有问题，我将words1.txt复制出一模一样的两份，也就是现在有三份相同的输入文件，再来跑一遍试试：<br><figure class="highlight console"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">root@hadoop-master:~/src/demo# hdfs dfs -rm -r /output</div><div class="line">17/09/30 04:48:37 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.</div><div class="line">Deleted /output</div><div class="line">root@hadoop-master:~/src/demo# ./run_script.sh /input /output mapper.py reducer.py</div><div class="line"></div><div class="line">Running python in Hadoop by hadoop streaming...</div></pre></td></tr></table></figure></p>
<p>这里我自己做了<code>run_script.sh</code>这样子一个shell script，用来缩短执行命令的长度，不然每次都要输入那么长真的好麻烦……<br><figure class="highlight console"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div></pre></td><td class="code"><pre><div class="line">root@hadoop-master:~/src/demo# hdfs dfs -cat /output/part*</div><div class="line">I   3</div><div class="line">Let 3</div><div class="line">bullshit    3</div><div class="line">down    3</div><div class="line">important   3</div><div class="line">is  6</div><div class="line">like    3</div><div class="line">looks   3</div><div class="line">me  3</div><div class="line">not 3</div><div class="line">something   9</div><div class="line">that    9</div><div class="line">trivial 3</div><div class="line">want    3</div><div class="line">what    3</div><div class="line">write   3</div><div class="line">yes 3</div></pre></td></tr></table></figure></p>
<p>看上去没有任何问题。</p>
<hr>
<h1 id="后记"><a href="#后记" class="headerlink" title="后记"></a>后记</h1><p>更多的关于Hadoop Streaming的内容，还希望大家去官网文档中查阅。例如像分配map和reduce的数量，设置partitioner，这样的参数都可以通过Hadoop Streaming来调整，还是很有意思的。</p>
<p>在下一篇关于MapReduce的文章中我会介绍一个相比较于word count复杂一点的例子，依然是用Python和Hadoop的结合。</p>
<hr>
<h1 id="Related-Links"><a href="#Related-Links" class="headerlink" title="Related Links"></a>Related Links</h1><ul>
<li><a href="http://www.michael-noll.com/tutorials/writing-an-hadoop-mapreduce-program-in-python/" target="_blank" rel="external">Writing an hadoop mapreduce program in python</a></li>
<li><a href="http://hadoop.apache.org/docs/r1.2.1/streaming.html#Hadoop+Streaming" target="_blank" rel="external">Hadoop Streaming Documentation</a></li>
</ul>
]]></content>
      
        <categories>
            
            <category> Big Data </category>
            
        </categories>
        
        
        <tags>
            
            <tag> Hadoop </tag>
            
            <tag> Python </tag>
            
            <tag> Docker </tag>
            
            <tag> MapReduce </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[Migrated from Heroku to Netlify]]></title>
      <url>/2017/09/28/Netlify/</url>
      <content type="html"><![CDATA[<p>之前学习ruby on rails的时候，开始接触的Heroku。这次部署个人博客，想着Heroku应该是个不错的选择吧，但试用了几天的Heroku之后，我决定还是转到其他地方部署我的blog，为什么呢？原因有二：</p>
<ol>
<li>免费的Heroku App，如果搁置时间长了一直无人访问的话，再次访问是需要一定时间等待激活的，就像是电脑睡眠了等着叫醒一样。付费的话太贵，对于只是搭载个人博客而言不太划算；</li>
<li>Heroku服务器是在美国和欧洲，而且一个App只能存在在某一个区域，不可变。所以对于国内的朋友，登录我的博客有时会有不小的延迟，虽然对于一个静态网站，本来也无需很快的响应速度，但配合第一条，这个时间有时真的让人无语……</li>
</ol>
<p>那之所以选择Netlify也是有两个原因：</p>
<ol>
<li>不存在Heroku的第一个问题；</li>
<li>静态内容部署采用的是global CDN方式，这样用户登录博客的时候会根据用户所在地选取最近的节点获取信息。</li>
</ol>
<p>除了这两点以外，Netlify的设置界面也做得十分简洁易用，相比Heroku而言，甚至连部署都更加简单，除了利用git部署，还可以直接把生成好的静态网页文件夹拖动到一个部署框内</p>
<p><img src="/images/drag_deploy.png" alt="drag_deploy"></p>
]]></content>
      
        <categories>
            
            <category> general </category>
            
        </categories>
        
        
        <tags>
            
            <tag> Netlify </tag>
            
            <tag> migrate </tag>
            
            <tag> Heroku </tag>
            
            <tag> static site </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[Hello World]]></title>
      <url>/2017/09/26/hello-world/</url>
      <content type="html"><![CDATA[<h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><p>终于将个人博客搭建起来了……虽然听说Hexo搭建博客十分容易，但对于前端完全不了解的我，颇费了一番功夫。</p>
<p>Hexo初始化这个博客目录时，自带了一个以“Hello World”为标题的文章，考虑到“Hello World”是作为学习编程代码第一课，作为一个标准码农的我就决定沿袭这个传统，<br>从此开始自己的博客之旅。</p>
<p><strong><em>Hello Hexo</em></strong></p>
<p><strong><em>Hello World</em></strong></p>
]]></content>
      
        <categories>
            
            <category> general </category>
            
        </categories>
        
        
        <tags>
            
            <tag> First </tag>
            
            <tag> Blog </tag>
            
        </tags>
        
    </entry>
    
  
  
</search>
